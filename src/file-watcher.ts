/**
 * Smart File Watcher - Incremental reindexing
 *
 * Watches for file changes and only reindexes modified files.
 * Saves ~80% indexation time by avoiding full rebuilds.
 */

import * as fs from "fs";
import * as path from "path";
import * as crypto from "crypto";
import { loadStore, saveStore, type VectorStore, type IndexedChunk } from "./store.js";
import { scanDirectory, type ScannedFile } from "./scanner.js";
import { chunkFile } from "./chunker.js";
import { initEmbedder, embed } from "./embedder.js";
import { getRagPath, ensureRagDir } from "./paths.js";

export interface FileHash {
  filePath: string;
  hash: string;
  mtime: number;
  size: number;
}

export interface HashIndex {
  version: string;
  createdAt: string;
  files: Record<string, FileHash>;
}

export interface WatcherStats {
  totalFiles: number;
  added: number;
  modified: number;
  deleted: number;
  unchanged: number;
  chunksUpdated: number;
  timeMs: number;
}

const HASH_VERSION = "1.0.0";

/**
 * Compute file hash
 */
function computeHash(content: string): string {
  return crypto.createHash("sha256").update(content).digest("hex").slice(0, 16);
}

/**
 * Load hash index
 */
export function loadHashIndex(rootDir: string): HashIndex {
  const hashPath = getRagPath(rootDir, "HASHES");

  if (!fs.existsSync(hashPath)) {
    return createEmptyHashIndex();
  }

  try {
    const data = fs.readFileSync(hashPath, "utf-8");
    const index = JSON.parse(data) as HashIndex;

    if (index.version !== HASH_VERSION) {
      return createEmptyHashIndex();
    }

    return index;
  } catch {
    return createEmptyHashIndex();
  }
}

/**
 * Save hash index
 */
export function saveHashIndex(rootDir: string, index: HashIndex): void {
  ensureRagDir(rootDir);
  const hashPath = getRagPath(rootDir, "HASHES");
  fs.writeFileSync(hashPath, JSON.stringify(index, null, 2));
}

/**
 * Create empty hash index
 */
function createEmptyHashIndex(): HashIndex {
  return {
    version: HASH_VERSION,
    createdAt: new Date().toISOString(),
    files: {},
  };
}

/**
 * Detect changed files by comparing current state to hash index
 */
export function detectChanges(
  files: ScannedFile[],
  hashIndex: HashIndex
): {
  added: ScannedFile[];
  modified: ScannedFile[];
  deleted: string[];
  unchanged: ScannedFile[];
} {
  const added: ScannedFile[] = [];
  const modified: ScannedFile[] = [];
  const unchanged: ScannedFile[] = [];
  const currentPaths = new Set<string>();

  for (const file of files) {
    currentPaths.add(file.relativePath);
    const existing = hashIndex.files[file.relativePath];
    const currentHash = computeHash(file.content);

    if (!existing) {
      added.push(file);
    } else if (existing.hash !== currentHash) {
      modified.push(file);
    } else {
      unchanged.push(file);
    }
  }

  // Find deleted files
  const deleted: string[] = [];
  for (const filePath of Object.keys(hashIndex.files)) {
    if (!currentPaths.has(filePath)) {
      deleted.push(filePath);
    }
  }

  return { added, modified, deleted, unchanged };
}

/**
 * Update hash index with current files
 */
export function updateHashIndex(
  hashIndex: HashIndex,
  files: ScannedFile[]
): void {
  for (const file of files) {
    const stats = fs.statSync(path.join(process.cwd(), file.relativePath));
    hashIndex.files[file.relativePath] = {
      filePath: file.relativePath,
      hash: computeHash(file.content),
      mtime: stats.mtimeMs,
      size: stats.size,
    };
  }
}

/**
 * Remove deleted files from hash index
 */
export function removeFromHashIndex(
  hashIndex: HashIndex,
  deletedPaths: string[]
): void {
  for (const filePath of deletedPaths) {
    delete hashIndex.files[filePath];
  }
}

/**
 * Incremental reindex - only process changed files
 */
export async function incrementalReindex(
  rootDir: string,
  options: { force?: boolean; useAST?: boolean } = {}
): Promise<WatcherStats> {
  const startTime = Date.now();
  const useAST = options.useAST !== false;

  // Load existing data
  const store = options.force ? null : loadStore(rootDir);
  const hashIndex = options.force ? createEmptyHashIndex() : loadHashIndex(rootDir);

  // Scan current files
  const files = await scanDirectory(rootDir);

  // Detect changes
  const { added, modified, deleted, unchanged } = detectChanges(files, hashIndex);

  // If no changes and we have a store, return early
  if (added.length === 0 && modified.length === 0 && deleted.length === 0 && store) {
    return {
      totalFiles: files.length,
      added: 0,
      modified: 0,
      deleted: 0,
      unchanged: unchanged.length,
      chunksUpdated: 0,
      timeMs: Date.now() - startTime,
    };
  }

  // Initialize embedder
  await initEmbedder();

  // Keep unchanged chunks from existing store
  const existingChunks = new Map<string, IndexedChunk[]>();
  if (store) {
    for (const chunk of store.chunks) {
      const existing = existingChunks.get(chunk.filePath) || [];
      existing.push(chunk);
      existingChunks.set(chunk.filePath, existing);
    }
  }

  // Process changed files
  const filesToProcess = [...added, ...modified];
  const newChunks: IndexedChunk[] = [];

  for (const file of filesToProcess) {
    const chunks = chunkFile(file.relativePath, file.content, { useAST });

    for (const chunk of chunks) {
      const embedding = await embed(chunk.content);
      newChunks.push({ ...chunk, embedding });
    }
  }

  // Combine: unchanged chunks + new chunks
  const allChunks: IndexedChunk[] = [];

  // Add unchanged file chunks
  for (const file of unchanged) {
    const fileChunks = existingChunks.get(file.relativePath) || [];
    allChunks.push(...fileChunks);
  }

  // Add new/modified chunks
  allChunks.push(...newChunks);

  // Remove deleted file chunks (already excluded by not adding them)

  // Save updated store
  const updatedStore: VectorStore = {
    version: "1.0.0",
    createdAt: new Date().toISOString(),
    chunks: allChunks,
  };
  saveStore(rootDir, updatedStore);

  // Update hash index
  updateHashIndex(hashIndex, [...added, ...modified, ...unchanged]);
  removeFromHashIndex(hashIndex, deleted);
  hashIndex.createdAt = new Date().toISOString();
  saveHashIndex(rootDir, hashIndex);

  return {
    totalFiles: files.length,
    added: added.length,
    modified: modified.length,
    deleted: deleted.length,
    unchanged: unchanged.length,
    chunksUpdated: newChunks.length,
    timeMs: Date.now() - startTime,
  };
}

/**
 * Quick check if reindex is needed (without full scan)
 */
export function needsReindex(rootDir: string): boolean {
  const hashIndex = loadHashIndex(rootDir);
  const store = loadStore(rootDir);

  if (!store) return true;
  if (Object.keys(hashIndex.files).length === 0) return true;

  // Quick mtime check on a sample of files
  const filePaths = Object.keys(hashIndex.files);
  const sampleSize = Math.min(10, filePaths.length);
  const sample = filePaths.slice(0, sampleSize);

  for (const filePath of sample) {
    const fullPath = path.join(rootDir, filePath);
    if (!fs.existsSync(fullPath)) {
      return true;
    }

    const stats = fs.statSync(fullPath);
    const cached = hashIndex.files[filePath];

    if (stats.mtimeMs > cached.mtime) {
      return true;
    }
  }

  return false;
}

/**
 * Get list of recently modified files
 */
export function getRecentlyModified(
  rootDir: string,
  sinceMs: number = 3600000 // 1 hour default
): string[] {
  const hashIndex = loadHashIndex(rootDir);
  const cutoff = Date.now() - sinceMs;
  const recent: string[] = [];

  for (const [filePath, info] of Object.entries(hashIndex.files)) {
    if (info.mtime > cutoff) {
      recent.push(filePath);
    }
  }

  return recent.sort((a, b) => {
    return hashIndex.files[b].mtime - hashIndex.files[a].mtime;
  });
}

/**
 * Format watcher stats for CLI
 */
export function formatWatcherStats(stats: WatcherStats): string {
  let output = "\nðŸ“Š Incremental Reindex Results\n\n";

  output += `Total files: ${stats.totalFiles}\n`;
  output += `Added: ${stats.added}\n`;
  output += `Modified: ${stats.modified}\n`;
  output += `Deleted: ${stats.deleted}\n`;
  output += `Unchanged: ${stats.unchanged}\n`;
  output += `Chunks updated: ${stats.chunksUpdated}\n`;
  output += `Time: ${stats.timeMs}ms\n`;

  const efficiency = stats.totalFiles > 0
    ? ((stats.unchanged / stats.totalFiles) * 100).toFixed(1)
    : "0.0";
  output += `\nðŸ’¡ Efficiency: ${efficiency}% files skipped (already indexed)\n`;

  return output;
}

/**
 * Watch directory for changes (one-shot check, not continuous)
 */
export async function checkForChanges(rootDir: string): Promise<{
  hasChanges: boolean;
  changes: {
    added: string[];
    modified: string[];
    deleted: string[];
  };
}> {
  const hashIndex = loadHashIndex(rootDir);
  const files = await scanDirectory(rootDir);
  const { added, modified, deleted } = detectChanges(files, hashIndex);

  return {
    hasChanges: added.length > 0 || modified.length > 0 || deleted.length > 0,
    changes: {
      added: added.map((f) => f.relativePath),
      modified: modified.map((f) => f.relativePath),
      deleted,
    },
  };
}
